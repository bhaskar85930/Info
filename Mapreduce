->How to Process 2 files in mapreduce : http://stackoverflow.com/questions/29637563/how-to-process-two-files-in-hadoop-mapreduce
->What are good ways to implement sort for 1 trillion files using map/reduce at Hadoop system? : 
The problem with one trillion files is related to a limitation of the NameNode. Since metadata is stored in memory, the actual RAM installed in the NameNode (or federated NameNodes) limits how many files can be stored on a stock HDFS cluster in the first place. I think you need something like 7 or 8 TB of RAM to handle a trillion files. Even if that weren't true, every file access has to go through the NameNode, which bottlenecks network traffic.

However, in a system like MapR, the metadata is distributed across the cluster (no NameNode at all) and there are no bottlenecks. You'll still need a pretty decent cluster to handle that many files. 

Provided you can store and process all those files, though, the approach would be pretty straightforward, much like doing any other sort in MapReduce, I would imagine.

->MapReduce program without mapper
IdentityMapper is a mapper which maps input directly to output.
If MapReduce programmer do not set the Mapper Class using JobConf.setMapperClass then IdentityMapper.class is used as a default value.

if you are not mentioning the mapper even then there will be one mapper running.so in any case atleast one mapper will be running.
->MapReduce program with Reducer
job.setNumReduceTasks(0);
 If you do not need sorting of map results - you set 0 reduced,and the job is called map only. 
If you need to sort the mapping results, but do not need any aggregation - you choose identity reducer. 
And to complete the picture we have a third case : we do need aggregation and, in this case we need reducer.

-> Mapreduce program with multiple outputs
The MultipleOutputs class simplifies writing output data to multiple outputs

Case one: writing to additional outputs other than the job default output. Each additional output, or named output, may be configured with its own OutputFormat, with its own key class and with its own value class.

Case two: to write data to different files provided by user

MultipleOutputs supports counters, by default they are disabled. The counters group is the MultipleOutputs class name. The names of the counters are the same as the output name. These count the number records written to each output name.
-> how to implement joins in mapreduce program
-> What is mapper, Reducer, partitioner, Combiner, Suffling, Sorting.
-> At which phase suffling takes place
-> At which place sorting takes place
-> How to write custom partitioner and combiner and usecase.
-> How to process binary file in mapreduce.
-> How to process sequence files in mapreduce
-> How to process CSV files in mapreduce
-> How to decide number of mappers
-> how to set number of reducers
-> What happens if no mapper and no reducer
-> What is logical and physical division of files
-> what is chunck size
-> How to set the size of split size
-> what is split size and record reader
-> Data types in mapreduce
-> what is serilazation
-> what is the difference between serlization in java and hadoop
-> diff between mapreduce 1 and mapeduce 2
-> What is the use of YARN and its componets like Resource manager, Node Manager, Application Manager, container.
-> what is job tracker, task tracker, name node, datanode, secondary namenode, backup node.
-> Rack Awareness, Speculative Execution.
-> where the intermediate data is stored in mapreduce program
-> How hadoop is different from traditional distributed systems
-> Advantages of mapreduce over Spark.
-> Explain in detail mapreduce2 program : https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html
-> What is Setup(), run() and clean() methods in mapreduce.
http://www.hadooptpoint.com/hadoop-setup-method-cleanup-method-example-in-mapreduce/
http://stackoverflow.com/questions/25432598/what-is-the-mapper-of-reducer-setup-used-for
-> 10 Useful Tips : http://blog.cloudera.com/blog/2009/05/10-mapreduce-tips/
-> How to process large number of small files in mapreduce. : http://blog.cloudera.com/blog/2009/02/the-small-files-problem/
-> What is Custom Record reader and usecase.




