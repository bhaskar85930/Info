
import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
//import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.RegionLocator;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.hadoop.hbase.KeyValue;


public class HbaseBulkLoadDriver extends Configured implements Tool {
	private static final String DATA_SEPERATOR = ",";
	private static final String TABLE_NAME = "/axp/replication_test/cluster_failover_test21";
	private static final String COLUMN_FAMILY_1 = "f1";
	private static final String COLUMN_FAMILY_2 = "f2";


	/**
	 * HBase bulk import example Data preparation MapReduce job driver
	 * 
	 * args[0]: HDFS input path 
	 * args[1]: HDFS output path
	 * 
	 */
	public static void main(String[] args)throws IOException, InterruptedException {
		try {
			int response = ToolRunner.run(HBaseConfiguration.create(), new HbaseBulkLoadDriver(), args);
			if (response == 0) {
				System.out.println("Job is successfully completed...");
			} else {
				System.out.println("Job failed...");
			}
		} catch (Exception exception) {
			exception.printStackTrace();
		}
	}

	@Override
	public int run(String[] args) throws Exception {
		int result = 0;
		String outputPath = args[1];
		Configuration configuration = getConf();
		configuration.set("data.seperator", DATA_SEPERATOR);
		configuration.set("hbase.table.name", TABLE_NAME);
		configuration.set("COLUMN_FAMILY_1", COLUMN_FAMILY_1);
		configuration.set("COLUMN_FAMILY_2", COLUMN_FAMILY_2);

		configuration.set("mapreduce.job.queuename", "root.replitest");
	
		Job job = Job.getInstance(configuration);
		job.setJarByClass(HbaseBulkLoadDriver.class);
		job.setJobName("Bulk Loading HBase Table::" + TABLE_NAME);
		job.setInputFormatClass(TextInputFormat.class);
		FileOutputFormat.setOutputPath(job, new Path(outputPath));
		HFileOutputFormat2.setOutputPath(job, new Path(outputPath));
		job.setOutputFormatClass(HFileOutputFormat2.class);
		FileInputFormat.addInputPaths(job, args[0]);
		FileSystem.getLocal(getConf()).delete(new Path(outputPath), true);
		job.setMapperClass(HbaseBulkLoadMapper.class);
		job.setMapOutputKeyClass(ImmutableBytesWritable.class);
		
		job.setMapOutputValueClass(KeyValue.class);

		Connection connection = ConnectionFactory.createConnection(configuration);
		Table table = connection.getTable(TableName.valueOf(TABLE_NAME));
		RegionLocator regionLocator = connection.getRegionLocator(TableName.valueOf(TABLE_NAME));
		HFileOutputFormat2.configureIncrementalLoad(job, table, regionLocator );
		
		
		job.waitForCompletion(true);
		if (job.isSuccessful()) {
			HbaseBulkLoad.doBulkLoad(outputPath, TABLE_NAME);
		} else {
			result = -1;
		}
		return result;
	}

}
